"use strict";(globalThis.webpackChunkaicraft_website=globalThis.webpackChunkaicraft_website||[]).push([[704],{5983(e,n,i){i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>h,frontMatter:()=>t,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"guides/edge-deployment","title":"Edge Deployment","description":"Deploy Aicraft models on resource-constrained devices using INT8 quantization and ARM NEON optimization.","source":"@site/docs/guides/edge-deployment.md","sourceDirName":"guides","slug":"/guides/edge-deployment","permalink":"/docs/guides/edge-deployment","draft":false,"unlisted":false,"editUrl":"https://github.com/TobiasTesauri/Aicraft/tree/main/docs/guides/edge-deployment.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"title":"Edge Deployment"},"sidebar":"docsSidebar","previous":{"title":"Training Guide","permalink":"/docs/guides/training"},"next":{"title":"Vulkan GPU","permalink":"/docs/guides/vulkan"}}');var a=i(4848),d=i(8453);const t={sidebar_position:2,title:"Edge Deployment"},s="Edge Deployment Guide",l={},c=[{value:"Why Edge Deployment?",id:"why-edge-deployment",level:2},{value:"Quantization Pipeline",id:"quantization-pipeline",level:2},{value:"1. Train in FP32",id:"1-train-in-fp32",level:3},{value:"2. Quantize Layers",id:"2-quantize-layers",level:3},{value:"3. Run Quantized Inference",id:"3-run-quantized-inference",level:3},{value:"4. Check Model Size",id:"4-check-model-size",level:3},{value:"How Quantization Works",id:"how-quantization-works",level:2},{value:"Calibration",id:"calibration",level:3},{value:"Quantize / Dequantize",id:"quantize--dequantize",level:3},{value:"Quantized GEMM",id:"quantized-gemm",level:3},{value:"ARM NEON Optimizations",id:"arm-neon-optimizations",level:2},{value:"GEMM Micro-kernel (NEON 6\xd78)",id:"gemm-micro-kernel-neon-68",level:3},{value:"Deployment Targets",id:"deployment-targets",level:2},{value:"Raspberry Pi 4 (ARM Cortex-A72)",id:"raspberry-pi-4-arm-cortex-a72",level:3},{value:"Android (NDK)",id:"android-ndk",level:3},{value:"Bare Metal / RTOS",id:"bare-metal--rtos",level:3},{value:"Best Practices",id:"best-practices",level:2}];function o(e){const n={admonition:"admonition",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,d.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"edge-deployment-guide",children:"Edge Deployment Guide"})}),"\n",(0,a.jsx)(n.p,{children:"Deploy Aicraft models on resource-constrained devices using INT8 quantization and ARM NEON optimization."}),"\n",(0,a.jsx)(n.h2,{id:"why-edge-deployment",children:"Why Edge Deployment?"}),"\n",(0,a.jsxs)(n.table,{children:[(0,a.jsx)(n.thead,{children:(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.th,{children:"Metric"}),(0,a.jsx)(n.th,{children:"Float32"}),(0,a.jsx)(n.th,{children:"INT8 Quantized"})]})}),(0,a.jsxs)(n.tbody,{children:[(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"Model size"}),(0,a.jsx)(n.td,{children:"Baseline"}),(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"~4\xd7 smaller"})})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"Memory bandwidth"}),(0,a.jsx)(n.td,{children:"Baseline"}),(0,a.jsx)(n.td,{children:(0,a.jsx)(n.strong,{children:"~4\xd7 less"})})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"Data type"}),(0,a.jsx)(n.td,{children:"32-bit float"}),(0,a.jsx)(n.td,{children:"8-bit unsigned int"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"Accumulation"}),(0,a.jsx)(n.td,{children:"FP32"}),(0,a.jsx)(n.td,{children:"INT32 (overflow-safe)"})]})]})]}),"\n",(0,a.jsx)(n.h2,{id:"quantization-pipeline",children:"Quantization Pipeline"}),"\n",(0,a.jsx)(n.h3,{id:"1-train-in-fp32",children:"1. Train in FP32"}),"\n",(0,a.jsx)(n.p,{children:"Train your model normally using standard floating-point precision:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-c",children:"ac_init();\r\n\r\nac_dense fc1, fc2;\r\nac_dense_init(&fc1, 784, 256);\r\nac_dense_init(&fc2, 256, 10);\r\n\r\n// ... training loop ...\n"})}),"\n",(0,a.jsx)(n.h3,{id:"2-quantize-layers",children:"2. Quantize Layers"}),"\n",(0,a.jsx)(n.p,{children:"Convert trained layers to INT8:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-c",children:"// Create quantized versions of each layer\r\nac_qdense qfc1, qfc2;\r\nac_qdense_from_dense(&qfc1, fc1.weight, fc1.bias, 784, 256);\r\nac_qdense_from_dense(&qfc2, fc2.weight, fc2.bias, 256, 10);\n"})}),"\n",(0,a.jsxs)(n.p,{children:["This performs per-tensor ",(0,a.jsx)(n.strong,{children:"asymmetric affine quantization"}),":"]}),"\n",(0,a.jsxs)(n.blockquote,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.code,{children:"q = round((x - min) / scale) + zero_point"})}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"Where:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.code,{children:"scale = (max - min) / 255"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.code,{children:"zero_point = round(-min / scale)"})}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"3-run-quantized-inference",children:"3. Run Quantized Inference"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-c",children:'ac_tensor* input = ac_tensor_2d(1, 784, 0);\r\nac_tensor_uniform(input, -1.0f, 1.0f);\r\n\r\n// Quantized forward pass\r\nac_tensor* hidden = ac_qdense_forward(&qfc1, input);\r\n\r\n// Apply activation on dequantized output\r\nfor (ac_size i = 0; i < hidden->shape.total_size; i++)\r\n    hidden->data[i] = hidden->data[i] > 0 ? hidden->data[i] : 0;  // relu\r\n\r\nac_tensor* output = ac_qdense_forward(&qfc2, hidden);\r\nac_tensor_print(output, "quantized prediction");\n'})}),"\n",(0,a.jsx)(n.h3,{id:"4-check-model-size",children:"4. Check Model Size"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-c",children:"ac_param_group params;\r\nac_param_group_init(&params);\r\nac_param_group_add(&params, fc1.weight);\r\nac_param_group_add(&params, fc1.bias);\r\nac_param_group_add(&params, fc2.weight);\r\nac_param_group_add(&params, fc2.bias);\r\n\r\nac_model_size_info info = ac_estimate_model_size(&params);\r\nac_print_model_size(&info);\r\n// Output:\r\n//   FP32 model size:   804.00 KB\r\n//   INT8 model size:   201.00 KB\r\n//   Compression ratio: 4.00\xd7\r\n\r\nac_param_group_destroy(&params);\n"})}),"\n",(0,a.jsx)(n.h2,{id:"how-quantization-works",children:"How Quantization Works"}),"\n",(0,a.jsx)(n.h3,{id:"calibration",children:"Calibration"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-c",children:"ac_quant_params qp;\r\nac_calibrate(float_data, size, &qp);\r\n// Scans data for min/max range\r\n// Ensures zero is representable (important for ReLU)\n"})}),"\n",(0,a.jsx)(n.h3,{id:"quantize--dequantize",children:"Quantize / Dequantize"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-c",children:"// Float32 \u2192 UINT8\r\nac_qtensor qt;\r\nac_quantize(float_tensor, &qt);\r\n\r\n// UINT8 \u2192 Float32\r\nac_tensor* recovered = ac_dequantize(&qt);\n"})}),"\n",(0,a.jsxs)(n.p,{children:["Both operations are ",(0,a.jsx)(n.strong,{children:"SIMD-accelerated"}),":"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"AVX2"}),": 8-wide vectorized pack/unpack (",(0,a.jsx)(n.code,{children:"_mm256_cvtps_epi32"})," \u2192 ",(0,a.jsx)(n.code,{children:"_mm_packs_epi32"})," \u2192 ",(0,a.jsx)(n.code,{children:"_mm_packus_epi16"}),")"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"NEON"}),": 4-wide vectorized with ",(0,a.jsx)(n.code,{children:"vcvtnq_s32_f32"})," \u2192 ",(0,a.jsx)(n.code,{children:"vmovn"})," narrowing"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"quantized-gemm",children:"Quantized GEMM"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-c",children:"// INT8 \xd7 INT8 \u2192 INT32 accumulation \u2192 Float32 output\r\nac_qgemm(&qA, &qB, output, M, N, K);\n"})}),"\n",(0,a.jsxs)(n.p,{children:["Uses ",(0,a.jsx)(n.strong,{children:"INT32 accumulation"})," to prevent overflow during matrix multiplication (a 256\xd7256 matmul could overflow INT16)."]}),"\n",(0,a.jsx)(n.h2,{id:"arm-neon-optimizations",children:"ARM NEON Optimizations"}),"\n",(0,a.jsx)(n.p,{children:"On ARM platforms, Aicraft provides real NEON intrinsics for:"}),"\n",(0,a.jsxs)(n.table,{children:[(0,a.jsx)(n.thead,{children:(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.th,{children:"Operation"}),(0,a.jsx)(n.th,{children:"NEON Implementation"})]})}),(0,a.jsxs)(n.tbody,{children:[(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"Element-wise add/mul/scale"}),(0,a.jsxs)(n.td,{children:[(0,a.jsx)(n.code,{children:"vaddq_f32"}),", ",(0,a.jsx)(n.code,{children:"vmulq_f32"}),", ",(0,a.jsx)(n.code,{children:"vmulq_n_f32"})]})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"FMA"}),(0,a.jsxs)(n.td,{children:[(0,a.jsx)(n.code,{children:"vfmaq_f32"})," (fused multiply-add)"]})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"Dot product"}),(0,a.jsx)(n.td,{children:"4-wide multiply-accumulate"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"ReLU"}),(0,a.jsx)(n.td,{children:(0,a.jsx)(n.code,{children:"vmaxq_f32(x, zero)"})})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"Exp"}),(0,a.jsx)(n.td,{children:"Cephes polynomial approximation"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"Sigmoid"}),(0,a.jsx)(n.td,{children:"Exp + Newton-Raphson reciprocal"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"Tanh"}),(0,a.jsx)(n.td,{children:(0,a.jsx)(n.code,{children:"2 \xd7 sigmoid(2x) - 1"})})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"GEMM"}),(0,a.jsx)(n.td,{children:"6\xd78 micro-kernel with 12 Q-register accumulators"})]})]})]}),"\n",(0,a.jsx)(n.h3,{id:"gemm-micro-kernel-neon-68",children:"GEMM Micro-kernel (NEON 6\xd78)"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-c",children:"// 12 accumulator registers (4 floats each)\r\nfloat32x4_t c[6][2];  // 6 rows \xd7 2 columns of 4-wide vectors\r\n\r\nfor (int k = 0; k < K; k++) {\r\n    float32x4_t b0 = vld1q_f32(&B[k * N + j]);\r\n    float32x4_t b1 = vld1q_f32(&B[k * N + j + 4]);\r\n    \r\n    for (int i = 0; i < 6; i++) {\r\n        c[i][0] = vfmaq_n_f32(c[i][0], b0, A[i * K + k]);\r\n        c[i][1] = vfmaq_n_f32(c[i][1], b1, A[i * K + k]);\r\n    }\r\n}\n"})}),"\n",(0,a.jsx)(n.h2,{id:"deployment-targets",children:"Deployment Targets"}),"\n",(0,a.jsx)(n.h3,{id:"raspberry-pi-4-arm-cortex-a72",children:"Raspberry Pi 4 (ARM Cortex-A72)"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Cross-compile for ARM\r\ncmake .. -DCMAKE_BUILD_TYPE=Release \\\r\n         -DCMAKE_C_COMPILER=aarch64-linux-gnu-gcc \\\r\n         -DCMAKE_CXX_COMPILER=aarch64-linux-gnu-g++\r\ncmake --build . --config Release\n"})}),"\n",(0,a.jsx)(n.h3,{id:"android-ndk",children:"Android (NDK)"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"cmake .. -DCMAKE_BUILD_TYPE=Release \\\r\n         -DCMAKE_TOOLCHAIN_FILE=$NDK/build/cmake/android.toolchain.cmake \\\r\n         -DANDROID_ABI=arm64-v8a \\\r\n         -DANDROID_NATIVE_API_LEVEL=21\r\ncmake --build .\n"})}),"\n",(0,a.jsx)(n.h3,{id:"bare-metal--rtos",children:"Bare Metal / RTOS"}),"\n",(0,a.jsx)(n.p,{children:"Since Aicraft has zero dependencies and uses only standard C, it can be compiled for any target with a C99 compiler. Disable the thread pool for single-core systems:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-c",children:"// In your build:\r\n// Don't compile thread_pool functionality\r\n// Use arena allocator with a fixed-size buffer\n"})}),"\n",(0,a.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,a.jsx)(n.admonition,{title:"Optimization Checklist",type:"tip",children:(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Train in FP32"})," \u2014 Don't quantize during training"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Quantize weights only"})," \u2014 Input is quantized on-the-fly per inference"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Check accuracy drop"})," \u2014 Compare FP32 vs INT8 output on your validation set"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Use arena allocator"})," \u2014 Eliminates ",(0,a.jsx)(n.code,{children:"malloc"}),"/",(0,a.jsx)(n.code,{children:"free"})," overhead on embedded"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Profile on target"})," \u2014 Measure actual latency, not just model size"]}),"\n"]})})]})}function h(e={}){const{wrapper:n}={...(0,d.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(o,{...e})}):o(e)}},8453(e,n,i){i.d(n,{R:()=>t,x:()=>s});var r=i(6540);const a={},d=r.createContext(a);function t(e){const n=r.useContext(d);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:t(e.components),r.createElement(d.Provider,{value:n},e.children)}}}]);