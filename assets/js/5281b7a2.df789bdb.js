"use strict";(globalThis.webpackChunkaicraft_website=globalThis.webpackChunkaicraft_website||[]).push([[443],{936(e,r,n){n.r(r),n.d(r,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>s,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"architecture","title":"Architecture","description":"Aicraft follows a layered architecture with zero-abstraction-cost design. Every component is a plain C struct with inline functions \u2014 no virtual dispatch, no heap allocation in hot paths.","source":"@site/docs/architecture.md","sourceDirName":".","slug":"/architecture","permalink":"/Aicraft/docs/architecture","draft":false,"unlisted":false,"editUrl":"https://github.com/TobiasTesauri/Aicraft/tree/main/docs/architecture.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"title":"Architecture"},"sidebar":"docsSidebar","previous":{"title":"Getting Started","permalink":"/Aicraft/docs/getting-started"},"next":{"title":"Training Guide","permalink":"/Aicraft/docs/guides/training"}}');var i=n(4848),a=n(8453);const s={sidebar_position:3,title:"Architecture"},o="Architecture",l={},c=[{value:"System Overview",id:"system-overview",level:2},{value:"Header Map",id:"header-map",level:2},{value:"Memory Management",id:"memory-management",level:2},{value:"Arena Allocator",id:"arena-allocator",level:3},{value:"Pool Allocator",id:"pool-allocator",level:3},{value:"GEMM Engine",id:"gemm-engine",level:2},{value:"Micro-kernels by Architecture",id:"micro-kernels-by-architecture",level:3},{value:"Autograd Engine",id:"autograd-engine",level:2},{value:"Vulkan Compute Pipeline",id:"vulkan-compute-pipeline",level:2},{value:"Data Flow: Training Step",id:"data-flow-training-step",level:2}];function d(e){const r={code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(r.header,{children:(0,i.jsx)(r.h1,{id:"architecture",children:"Architecture"})}),"\n",(0,i.jsx)(r.p,{children:"Aicraft follows a layered architecture with zero-abstraction-cost design. Every component is a plain C struct with inline functions \u2014 no virtual dispatch, no heap allocation in hot paths."}),"\n",(0,i.jsx)(r.h2,{id:"system-overview",children:"System Overview"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502                    User Application                     \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502   Layers   \u2502    Loss    \u2502  Optimizer   \u2502  Serialize     \u2502\r\n\u2502  Dense     \u2502  MSE       \u2502  SGD/Adam    \u2502  Save/Load     \u2502\r\n\u2502  Conv2D    \u2502  CE/BCE    \u2502  LR Schedule \u2502  .acml format  \u2502\r\n\u2502  BatchNorm \u2502            \u2502  Grad Clip   \u2502                \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502                   Autograd Engine                        \u2502\r\n\u2502        Reverse-mode AD \xb7 22 ops \xb7 Dynamic graph          \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502    Tensor Core       \u2502         Quantization             \u2502\r\n\u2502  N-D tensors (\u22648D)   \u2502   INT8 affine \xb7 QGEMM           \u2502\r\n\u2502  SIMD-aligned data   \u2502   ~4\xd7 compression               \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502                  Compute Backends                        \u2502\r\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\r\n\u2502  \u2502   CPU / SIMD     \u2502  \u2502      Vulkan GPU              \u2502  \u2502\r\n\u2502  \u2502  AVX-512/AVX2    \u2502  \u2502  14 compute shaders          \u2502  \u2502\r\n\u2502  \u2502  SSE / NEON      \u2502  \u2502  Shared-mem tiled GEMM       \u2502  \u2502\r\n\u2502  \u2502  BLIS-style GEMM \u2502  \u2502  Auto-dispatch (size-based)  \u2502  \u2502\r\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502                   Systems Layer                          \u2502\r\n\u2502   Arena/Pool Memory  \xb7  Thread Pool  \xb7  Error Handling   \u2502\r\n\u2502   Platform Detection \xb7  PRNG (xoshiro128**)              \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,i.jsx)(r.h2,{id:"header-map",children:"Header Map"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{children:"include/aicraft/\r\n\u251c\u2500\u2500 platform.h       # Platform detection, SIMD macros, compiler hints\r\n\u251c\u2500\u2500 error.h          # Error codes, handlers, reporting macros\r\n\u251c\u2500\u2500 memory.h         # Arena & pool allocators, checkpoint/restore\r\n\u251c\u2500\u2500 simd_math.h      # SIMD kernels: GEMM, dot, relu, exp, etc.\r\n\u251c\u2500\u2500 fast_math.h      # Approximated exp, tanh (polynomial fits)\r\n\u251c\u2500\u2500 tensor.h         # Tensor structure with autograd metadata\r\n\u251c\u2500\u2500 tensor_ops.h     # Tensor operations (add, sub, mul, matmul, reshape)\r\n\u251c\u2500\u2500 autograd.h       # Reverse-mode autodiff engine (dynamic graph)\r\n\u251c\u2500\u2500 layers.h         # Dense, Conv2D, BatchNorm, Dropout, MaxPool\r\n\u251c\u2500\u2500 loss.h           # MSE, CrossEntropy, BCE losses\r\n\u251c\u2500\u2500 optimizer.h      # SGD/Adam/AdamW + grad clipping + LR schedulers\r\n\u251c\u2500\u2500 serialize.h      # Binary model save/load\r\n\u251c\u2500\u2500 quantize.h       # INT8 quantization engine\r\n\u251c\u2500\u2500 vulkan.h         # Vulkan compute backend\r\n\u251c\u2500\u2500 thread_pool.h    # Thread pool for parallel GEMM\r\n\u2514\u2500\u2500 aicraft.h        # Single-header include + lifecycle\n"})}),"\n",(0,i.jsx)(r.h2,{id:"memory-management",children:"Memory Management"}),"\n",(0,i.jsx)(r.p,{children:"Aicraft uses a two-tier memory system designed to eliminate per-tensor allocation overhead:"}),"\n",(0,i.jsx)(r.h3,{id:"arena-allocator",children:"Arena Allocator"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{children:"Training loop with checkpoint/restore:\r\n\r\nac_arena_save()     \u2190\u2500\u2500 save pointer position\r\n  \u2502\r\n  \u251c\u2500\u2500 forward pass allocations (intermediates)\r\n  \u251c\u2500\u2500 backward pass allocations (gradients)\r\n  \u251c\u2500\u2500 optimizer temporaries\r\n  \u2502\r\nac_arena_restore()  \u2190\u2500\u2500 reset pointer, all intermediates freed instantly\n"})}),"\n",(0,i.jsx)(r.p,{children:(0,i.jsx)(r.strong,{children:"Key properties:"})}),"\n",(0,i.jsxs)(r.ul,{children:["\n",(0,i.jsxs)(r.li,{children:["Single ",(0,i.jsx)(r.code,{children:"malloc"})," for 64 MB blocks, bump-pointer sub-allocation"]}),"\n",(0,i.jsx)(r.li,{children:"SIMD-aligned (64-byte) by default"}),"\n",(0,i.jsxs)(r.li,{children:["O(1) bulk deallocation via ",(0,i.jsx)(r.code,{children:"restore()"})]}),"\n",(0,i.jsxs)(r.li,{children:["Model parameters allocated ",(0,i.jsx)(r.em,{children:"before"})," the checkpoint survive restore"]}),"\n"]}),"\n",(0,i.jsx)(r.h3,{id:"pool-allocator",children:"Pool Allocator"}),"\n",(0,i.jsx)(r.p,{children:"Fixed-size free-list allocator for uniform objects (e.g., autograd nodes). O(1) alloc and free with bounds checking."}),"\n",(0,i.jsx)(r.h2,{id:"gemm-engine",children:"GEMM Engine"}),"\n",(0,i.jsx)(r.p,{children:"The GEMM implementation follows the BLIS/GotoBLAS algorithm:"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{children:"For each block of N (NC=4096):\r\n  Pack B panel \u2192 B\u0303  (KC \xd7 NC, column-contiguous)\r\n  For each block of M (MC=72):\r\n    Pack A panel \u2192 \xc3  (MC \xd7 KC, row-contiguous)\r\n    For each micro-tile:\r\n      Micro-kernel: \xc3[MR\xd7KC] \xd7 B\u0303[KC\xd7NR] \u2192 C[MR\xd7NR]\n"})}),"\n",(0,i.jsx)(r.h3,{id:"micro-kernels-by-architecture",children:"Micro-kernels by Architecture"}),"\n",(0,i.jsxs)(r.table,{children:[(0,i.jsx)(r.thead,{children:(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.th,{children:"Architecture"}),(0,i.jsx)(r.th,{children:"Tile Size"}),(0,i.jsx)(r.th,{children:"Registers"}),(0,i.jsx)(r.th,{children:"FMAs/cycle"})]})}),(0,i.jsxs)(r.tbody,{children:[(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"AVX-512"}),(0,i.jsx)(r.td,{children:"6\xd732"}),(0,i.jsx)(r.td,{children:"12 ZMM + 2 load"}),(0,i.jsx)(r.td,{children:"192/k-step"})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"AVX2"}),(0,i.jsx)(r.td,{children:"6\xd716"}),(0,i.jsx)(r.td,{children:"12 YMM + 2 load"}),(0,i.jsx)(r.td,{children:"96/k-step"})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"ARM NEON"}),(0,i.jsx)(r.td,{children:"6\xd78"}),(0,i.jsx)(r.td,{children:"12 Q-reg"}),(0,i.jsx)(r.td,{children:"48/k-step"})]}),(0,i.jsxs)(r.tr,{children:[(0,i.jsx)(r.td,{children:"Scalar"}),(0,i.jsx)(r.td,{children:"6\xd78"}),(0,i.jsx)(r.td,{children:"\u2014"}),(0,i.jsx)(r.td,{children:"Baseline"})]})]})]}),"\n",(0,i.jsx)(r.p,{children:(0,i.jsx)(r.strong,{children:"Optimizations:"})}),"\n",(0,i.jsxs)(r.ul,{children:["\n",(0,i.jsx)(r.li,{children:"L1 prefetch hints every 4 K iterations (AVX2)"}),"\n",(0,i.jsx)(r.li,{children:"Thread-parallel outer loop for M \u2265 2\xd7MC"}),"\n",(0,i.jsx)(r.li,{children:"Panel packing for cache-line-aligned access"}),"\n",(0,i.jsxs)(r.li,{children:["FMA instructions when available (",(0,i.jsx)(r.code,{children:"__FMA__"})," detection)"]}),"\n"]}),"\n",(0,i.jsx)(r.h2,{id:"autograd-engine",children:"Autograd Engine"}),"\n",(0,i.jsx)(r.p,{children:"The autograd engine implements reverse-mode automatic differentiation:"}),"\n",(0,i.jsxs)(r.ol,{children:["\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"Forward pass"}),": Each operation records its inputs and type in the tensor's ",(0,i.jsx)(r.code,{children:"op"})," field"]}),"\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"Topological sort"}),": DFS from the loss tensor, collecting all dependencies"]}),"\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"Backward pass"}),": Walk the sorted list in reverse, computing gradients"]}),"\n"]}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{children:"Loss tensor (seed grad = 1.0)\r\n    \u2502\r\n    \u25bc\r\nTopological Sort (DFS)\r\n    \u2502\r\n    \u25bc\r\n[loss, softmax, matmul, relu, matmul, ...]  \u2190 reverse order\r\n    \u2502\r\n    \u25bc\r\nFor each op: dispatch backward rule (22 op types)\r\n    \u2502\r\n    \u25bc\r\nAll parameter .grad fields populated\n"})}),"\n",(0,i.jsxs)(r.p,{children:[(0,i.jsx)(r.strong,{children:"O(1) visited check"}),": A global epoch counter increments per backward call. Each tensor stores the epoch it was last visited \u2014 no hash set needed."]}),"\n",(0,i.jsx)(r.h2,{id:"vulkan-compute-pipeline",children:"Vulkan Compute Pipeline"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{children:"Host                          GPU\r\n\u2500\u2500\u2500\u2500\u2500                         \u2500\u2500\u2500\u2500\r\nac_vk_init()            \u2192     Create instance, device, queues\r\nac_vk_create_buffer()   \u2192     Allocate STORAGE_BUFFER\r\nac_vk_upload()          \u2192     Staging buffer \u2192 GPU transfer\r\nac_vk_gemm()            \u2192     Dispatch GEMM shader (16\xd716 tiles)\r\nac_vk_download()        \u2192     GPU \u2192 staging buffer \u2192 host\n"})}),"\n",(0,i.jsxs)(r.ul,{children:["\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"14 compute shaders"}),": GEMM, add, mul, scale, FMA, ReLU (fwd+bwd), sigmoid (fwd+bwd), tanh (fwd+bwd), softmax, sum"]}),"\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"Auto-dispatch"}),": Operations on tensors \u22654096 elements go to GPU; smaller to CPU SIMD"]}),"\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.strong,{children:"Dynamic loading"}),": Vulkan functions loaded at runtime (no compile-time SDK dependency)"]}),"\n"]}),"\n",(0,i.jsx)(r.h2,{id:"data-flow-training-step",children:"Data Flow: Training Step"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{children:"1. ac_zero_grad()           Clear all .grad buffers\r\n2. ac_dense_forward()       GEMM \u2192 bias add \u2192 store autograd info\r\n3. ac_tensor_relu()         Element-wise ReLU \u2192 record op\r\n4. ac_cross_entropy_loss()  Fused softmax+CE \u2192 scalar loss\r\n5. ac_backward()            Topo-sort \u2192 reverse backward rules\r\n6. ac_clip_grad_norm()      SIMD L2-norm compute \u2192 scale gradients\r\n7. ac_adam_step()            SIMD-vectorized parameter update\r\n8. ac_arena_restore()       Bulk-free all intermediates\n"})})]})}function h(e={}){const{wrapper:r}={...(0,a.R)(),...e.components};return r?(0,i.jsx)(r,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453(e,r,n){n.d(r,{R:()=>s,x:()=>o});var t=n(6540);const i={},a=t.createContext(i);function s(e){const r=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(r):{...r,...e}},[r,e])}function o(e){let r;return r=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),t.createElement(a.Provider,{value:r},e.children)}}}]);