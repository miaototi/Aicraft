"use strict";(globalThis.webpackChunkaicraft_website=globalThis.webpackChunkaicraft_website||[]).push([[959],{4535(e,n,r){r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>p,frontMatter:()=>c,metadata:()=>a,toc:()=>t});const a=JSON.parse('{"id":"guides/training","title":"Training Guide","description":"Complete walkthrough for training neural networks with Aicraft, from model definition to checkpoint saving.","source":"@site/docs/guides/training.md","sourceDirName":"guides","slug":"/guides/training","permalink":"/docs/guides/training","draft":false,"unlisted":false,"editUrl":"https://github.com/TobiasTesauri/Aicraft/tree/main/docs/guides/training.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"title":"Training Guide"},"sidebar":"docsSidebar","previous":{"title":"Architecture","permalink":"/docs/architecture"},"next":{"title":"Edge Deployment","permalink":"/docs/guides/edge-deployment"}}');var i=r(4848),s=r(8453);const c={sidebar_position:1,title:"Training Guide"},o="Training Guide",l={},t=[{value:"Model Definition",id:"model-definition",level:2},{value:"Parameter Registration",id:"parameter-registration",level:2},{value:"Optimizers",id:"optimizers",level:2},{value:"Adam (recommended)",id:"adam-recommended",level:3},{value:"AdamW (with decoupled weight decay)",id:"adamw-with-decoupled-weight-decay",level:3},{value:"SGD (with momentum)",id:"sgd-with-momentum",level:3},{value:"Training Loop",id:"training-loop",level:2},{value:"Loss Functions",id:"loss-functions",level:2},{value:"Cross-Entropy (classification)",id:"cross-entropy-classification",level:3},{value:"Mean Squared Error (regression)",id:"mean-squared-error-regression",level:3},{value:"Binary Cross-Entropy",id:"binary-cross-entropy",level:3},{value:"Gradient Clipping",id:"gradient-clipping",level:2},{value:"Global L2-norm clipping",id:"global-l2-norm-clipping",level:3},{value:"Per-element value clipping",id:"per-element-value-clipping",level:3},{value:"Learning Rate Schedulers",id:"learning-rate-schedulers",level:2},{value:"Saving and Loading",id:"saving-and-loading",level:2},{value:"Convolutional Networks",id:"convolutional-networks",level:2},{value:"Cleanup",id:"cleanup",level:2}];function d(e){const n={admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",p:"p",pre:"pre",strong:"strong",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"training-guide",children:"Training Guide"})}),"\n",(0,i.jsx)(n.p,{children:"Complete walkthrough for training neural networks with Aicraft, from model definition to checkpoint saving."}),"\n",(0,i.jsx)(n.h2,{id:"model-definition",children:"Model Definition"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-c",children:"#include <aicraft/aicraft.h>\r\n\r\nint main() {\r\n    ac_init();\r\n\r\n    // Define layers\r\n    ac_dense fc1, fc2, fc3;\r\n    ac_dense_init(&fc1, 784, 256);   // Input \u2192 Hidden 1\r\n    ac_dense_init(&fc2, 256, 128);   // Hidden 1 \u2192 Hidden 2\r\n    ac_dense_init(&fc3, 128, 10);    // Hidden 2 \u2192 Output\n"})}),"\n",(0,i.jsxs)(n.p,{children:["All layers use ",(0,i.jsx)(n.strong,{children:"He initialization"})," by default, which is optimal for ReLU activations."]}),"\n",(0,i.jsx)(n.h2,{id:"parameter-registration",children:"Parameter Registration"}),"\n",(0,i.jsx)(n.p,{children:"Before training, register all learnable parameters in a parameter group:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-c",children:"    ac_param_group params;\r\n    ac_param_group_init(&params);\r\n    \r\n    // Add weights and biases from each layer\r\n    ac_param_group_add(&params, fc1.weight);\r\n    ac_param_group_add(&params, fc1.bias);\r\n    ac_param_group_add(&params, fc2.weight);\r\n    ac_param_group_add(&params, fc2.bias);\r\n    ac_param_group_add(&params, fc3.weight);\r\n    ac_param_group_add(&params, fc3.bias);\n"})}),"\n",(0,i.jsx)(n.p,{children:"The parameter group dynamically grows \u2014 no static limit on the number of parameters."}),"\n",(0,i.jsx)(n.h2,{id:"optimizers",children:"Optimizers"}),"\n",(0,i.jsx)(n.h3,{id:"adam-recommended",children:"Adam (recommended)"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-c",children:"    ac_adam opt;\r\n    ac_adam_init(&opt, &params,\r\n        0.001f,   // learning rate\r\n        0.9f,     // beta1\r\n        0.999f,   // beta2\r\n        1e-8f,    // epsilon\r\n        0.0f,     // weight decay (0 = no decay)\r\n        0         // timestep (0 = start fresh)\r\n    );\n"})}),"\n",(0,i.jsx)(n.h3,{id:"adamw-with-decoupled-weight-decay",children:"AdamW (with decoupled weight decay)"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-c",children:"    ac_adam opt;\r\n    ac_adam_init(&opt, &params, 0.001f, 0.9f, 0.999f, 1e-8f, 0.01f, 0);\r\n    // weight_decay=0.01 enables AdamW behavior\n"})}),"\n",(0,i.jsx)(n.h3,{id:"sgd-with-momentum",children:"SGD (with momentum)"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-c",children:"    ac_sgd opt;\r\n    ac_sgd_init(&opt, &params,\r\n        0.01f,    // learning rate\r\n        0.9f,     // momentum\r\n        0.0001f   // weight decay\r\n    );\n"})}),"\n",(0,i.jsxs)(n.p,{children:["All optimizer step functions are ",(0,i.jsx)(n.strong,{children:"SIMD-accelerated"})," (AVX2/NEON vectorized parameter updates)."]}),"\n",(0,i.jsx)(n.h2,{id:"training-loop",children:"Training Loop"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-c",children:'    for (int epoch = 0; epoch < 100; epoch++) {\r\n        // Checkpoint arena \u2014 all intermediates will be freed at restore\r\n        ac_arena_checkpoint cp;\r\n        ac_arena_save(&g_tensor_arena, &cp);\r\n        \r\n        // Zero gradients\r\n        ac_zero_grad(&params);\r\n        \r\n        // Forward pass\r\n        ac_tensor* h = ac_dense_forward(&fc1, input);\r\n        h = ac_tensor_relu(h);\r\n        h = ac_dense_forward(&fc2, h);\r\n        h = ac_tensor_relu(h);\r\n        ac_tensor* logits = ac_dense_forward(&fc3, h);\r\n        \r\n        // Compute loss\r\n        ac_tensor* loss = ac_cross_entropy_loss(logits, labels);\r\n        \r\n        // Backward pass\r\n        ac_backward(loss);\r\n        \r\n        // Gradient clipping (prevents exploding gradients)\r\n        ac_clip_grad_norm(&params, 1.0f);\r\n        \r\n        // Optimizer step\r\n        ac_adam_step(&opt);\r\n        \r\n        // Free all intermediates (model params survive)\r\n        ac_arena_restore(&g_tensor_arena, &cp);\r\n        \r\n        if (epoch % 10 == 0) {\r\n            printf("Epoch %d, Loss: %.4f\\n", epoch, loss->data[0]);\r\n        }\r\n    }\n'})}),"\n",(0,i.jsx)(n.admonition,{title:"Arena Checkpointing",type:"tip",children:(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"ac_arena_save"})," / ",(0,i.jsx)(n.code,{children:"ac_arena_restore"})," pattern is critical: it prevents memory growth during training. All intermediate tensors (activations, gradients) are bulk-freed in O(1) each epoch. Model parameters are allocated ",(0,i.jsx)(n.em,{children:"before"})," the checkpoint so they survive."]})}),"\n",(0,i.jsx)(n.h2,{id:"loss-functions",children:"Loss Functions"}),"\n",(0,i.jsx)(n.h3,{id:"cross-entropy-classification",children:"Cross-Entropy (classification)"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-c",children:"ac_tensor* loss = ac_cross_entropy_loss(logits, labels);\r\n// logits: [batch, classes], labels: [batch, classes] (one-hot)\n"})}),"\n",(0,i.jsx)(n.h3,{id:"mean-squared-error-regression",children:"Mean Squared Error (regression)"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-c",children:"ac_tensor* loss = ac_mse_loss(predictions, targets);\n"})}),"\n",(0,i.jsx)(n.h3,{id:"binary-cross-entropy",children:"Binary Cross-Entropy"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-c",children:"ac_tensor* loss = ac_bce_loss(sigmoid_output, binary_labels);\n"})}),"\n",(0,i.jsx)(n.h2,{id:"gradient-clipping",children:"Gradient Clipping"}),"\n",(0,i.jsx)(n.h3,{id:"global-l2-norm-clipping",children:"Global L2-norm clipping"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-c",children:"ac_clip_grad_norm(&params, 1.0f);  // clip to max norm of 1.0\n"})}),"\n",(0,i.jsx)(n.h3,{id:"per-element-value-clipping",children:"Per-element value clipping"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-c",children:"ac_clip_grad_value(&params, 0.5f);  // clip each grad element to [-0.5, 0.5]\n"})}),"\n",(0,i.jsx)(n.h2,{id:"learning-rate-schedulers",children:"Learning Rate Schedulers"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-c",children:"ac_lr_scheduler sched;\r\n\r\n// Step decay: lr *= gamma every step_size epochs\r\nac_lr_scheduler_init(&sched, AC_LR_STEP, base_lr, step_size, gamma, 0, 0);\r\n\r\n// Cosine annealing: lr follows cosine curve\r\nac_lr_scheduler_init(&sched, AC_LR_COSINE, base_lr, 0, 0, total_epochs, min_lr);\r\n\r\n// Exponential decay: lr *= gamma every epoch\r\nac_lr_scheduler_init(&sched, AC_LR_EXPONENTIAL, base_lr, 0, gamma, 0, 0);\r\n\r\n// In training loop:\r\nfloat lr = ac_lr_scheduler_get(&sched, epoch);\r\nopt.lr = lr;  // update optimizer\n"})}),"\n",(0,i.jsx)(n.h2,{id:"saving-and-loading",children:"Saving and Loading"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-c",children:'    // Save trained model\r\n    ac_model_save("model.acml", &params);\r\n    \r\n    // Load model\r\n    ac_error_code err = ac_model_load("model.acml", &params);\r\n    if (err != AC_OK) {\r\n        printf("Load failed: %s\\n", ac_get_last_error_message());\r\n    }\n'})}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:".acml"})," format uses a versioned binary format with a magic header for corruption detection."]}),"\n",(0,i.jsx)(n.h2,{id:"convolutional-networks",children:"Convolutional Networks"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-c",children:"    ac_conv2d conv1;\r\n    ac_conv2d_init(&conv1, \r\n        1,    // in_channels\r\n        32,   // out_channels\r\n        3,    // kernel_size\r\n        1,    // stride\r\n        1     // padding\r\n    );\r\n    \r\n    ac_maxpool2d pool1;\r\n    ac_maxpool2d_init(&pool1, 2, 2);  // 2\xd72 pool, stride 2\r\n    \r\n    ac_batchnorm bn1;\r\n    ac_batchnorm_init(&bn1, 32);  // 32 channels\r\n    \r\n    // Forward\r\n    ac_tensor* x = ac_conv2d_forward(&conv1, input);    // im2col + GEMM\r\n    x = ac_tensor_relu(x);\r\n    x = ac_maxpool2d_forward(&pool1, x);\r\n    x = ac_batchnorm_forward(&bn1, x, 1);               // 1 = training mode\r\n    x = ac_flatten_forward(x);                            // zero-copy view\r\n    x = ac_dense_forward(&fc1, x);\n"})}),"\n",(0,i.jsx)(n.h2,{id:"cleanup",children:"Cleanup"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-c",children:"    ac_param_group_destroy(&params);\r\n    ac_cleanup();\r\n    return 0;\r\n}\n"})})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453(e,n,r){r.d(n,{R:()=>c,x:()=>o});var a=r(6540);const i={},s=a.createContext(i);function c(e){const n=a.useContext(s);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:c(e.components),a.createElement(s.Provider,{value:n},e.children)}}}]);